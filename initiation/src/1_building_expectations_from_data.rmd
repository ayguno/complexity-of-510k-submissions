---
title: "Building expectations from the public 510(k) database"
author: "Ozan Aygun"
date: "9/14/2022"
output: 
  html_document:
    code_folding: show                  
    depth: 6
    df_print: paged
    highlight: tango
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float:
      collapsed: True
      smooth_scroll: True
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",dev = 'svg',
                      fig.height= 8, fig.width= 8,message=FALSE,warning=FALSE)
```


```{r library loading}
require(dplyr)
require(pheatmap)
require(stringr)
require(reshape2)
require(caret)
require(lubridate)
require(readxl)
require(ggplot2)
require(quanteda)
require(corrplot)
require(philentropy)
require(jsonlite)

# Ingest the configuration file
config <- fromJSON("config.json")
```


## Goals

1. Reasonably automate ingestion of the most recent 510(k) data from the public database:

https://www.fda.gov/medical-devices/510k-clearances/downloadable-510k-files

2. Make the analytical decision: when to start an analytical data set? e.g: MDUFA 1 ?

3. Make the analytical decision: focus only on traditional 510(k)s.

4. Define response variable for the classification problem.

5. Explore and engineer potential features that could be useful for the problem. (Note that these would be relatively simple features that could be engineered using a small subset, without causing data leakage. More complex feature engineering that requires a training pipeline will be performed in modeling experiments).

6. Document next steps for automation and experimentation


## Reasonably automate ingestion of the most recent 510(k) data from the public database


```{r data ingestion}
# download the most recent data (it comes zipped)
download.file(url = config$most_recent_501k_data_path,
              destfile = "../data/most_recent_data.zip")
# unzip
unzip("../data/most_recent_data.zip",exdir = "../data")
# delete compressed directory
unlink("../data/most_recent_data.zip")
data.filename <- gsub("\\.zip","\\.txt",basename(config$most_recent_501k_data_path)) 

# load the flat file
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)
```


## Make the analytical decision: when to start an analytical data set?

Data dictionary for the main dataset: 

https://www.fda.gov/medical-devices/510k-clearances/file-layout-releasable-510ks



```{r data cut off, fig.width=15, fig.height=5}
# load the flat file
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

# define DECISION_TIME_DAYS
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS)
         )

data_summary <- data_510k %>%
  mutate(MARK_MONTH = floor_date(DECISIONDATE,"month"))%>%
  group_by(MARK_MONTH)%>%
  summarise(MEDIAN_DECISION_TIME_MONTHLY = median(DECISION_TIME_DAYS_LOG10))

ggplot(data_510k %>%
         filter(THIRDPARTY == 'N' & TYPE == "Traditional" & DECISION == "SESE") %>%
         mutate(MARK_MONTH = floor_date(DECISIONDATE,"month"))%>%
         left_join(data_summary),aes(x = DECISIONDATE, y = DECISION_TIME_DAYS_LOG10))+
  geom_point(color = "navy", alpha = 0.05, size = 0.9)+
  geom_line(aes(x = MARK_MONTH, y = MEDIAN_DECISION_TIME_MONTHLY), color = "magenta", size = 2)+ 
  geom_vline(xintercept = mdy("10-01-2002"), color = "red")+ # https://www.fda.gov/industry/fda-user-fee-programs/medical-device-user-fee-amendments-mdufa
  geom_vline(xintercept = mdy("10-01-2007"), color = "red")+ # MDUFA II
  geom_vline(xintercept = mdy("10-01-2012"), color = "red")+ # MDUFA III
  geom_vline(xintercept = mdy("10-01-2017"), color = "red")+ # MDUFA IV
  theme_bw()+
  theme(panel.background = element_rect(fill = "#f7edfa"))
```

Based on the observations, it would be sensible to limit the analytical data set starting from FY 2007 (MDUFA II and beyond).


## Make the analytical decision: focus only on traditional 510(k)s


What fraction of data we would be filtering when we: 

- Remove special 510(k)s
- Remove DeNovos and only keep SESE decisions
- Remove Third Party 510(k)s



```{r data filtering and its impact, fig.width=15, fig.height=5}
# load the flat file
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE) %>%
  mutate(DECISIONDATE = mdy(DECISIONDATE))

summarize_data <- function(att_name,df){
  data_summary <- table(df[att_name]) %>% 
  rbind(100*table(df[att_name])/nrow(df))
  rownames(data_summary) <- c("Total","Percent")
  print(att_name)
  print(round(data_summary,2))
}

# Distribution of different 510(k)s and decision codes
summarize_data(att_name = "THIRDPARTY", df = data_510k %>% filter(DECISIONDATE >= mdy("10-01-2007")))
summarize_data(att_name = "TYPE", df = data_510k %>% filter(DECISIONDATE >= mdy("10-01-2007")))
summarize_data(att_name = "DECISION", df = data_510k %>% filter(DECISIONDATE >= mdy("10-01-2007")))



data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

# append into processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS)
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") # Retain records with SESE decisions
         

print(paste0(nrow(data_510k), " traditional, non-third party 510(k) records with SESE decision codes and decision date after FY 2007."))
```


## Define response variable for classification


```{r response variable definition}
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

# established processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS)
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") # Retain records with SESE decisions

hist(data_510k$DECISION_TIME_DAYS_LOG10, breaks = 250, col = "navy", border = "navy")
abline(v = log10(90), col = "magenta")
abline(v = log10(265), col = "magenta")
plot(density(data_510k$DECISION_TIME_DAYS_LOG10), col = "navy", lwd = 2)
abline(v = log10(90), col = "magenta")
abline(v = log10(265), col = "magenta")
```

It appears that decision time displays a bimodal distribution (perhaps another mode is emerging between 1.5 - 2), one corresponds to day 90 and another corresponds to day 265. Based on this distribution, we may try to classify 510(k) submission COMPLEXITY as:

- LOW (L): low complexity, DECISION_TIME_DAYS <= 90
- MEDIUM (M): medium complexity, 90 < DECISION_TIME_DAYS <= 265
- HIGH (H): high complexity, 265 < DECISION_TIME_DAYS 


```{r response variable complexity definition, fig.height= 6, fig.width=15}
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

# append into processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS),
         COMPLEXITY = ifelse(DECISION_TIME_DAYS <= 90,"L",
                             ifelse(DECISION_TIME_DAYS <= 265,"M","H")) # Define COMPLEXITY based on DECISION_TIME_DAYS
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") # Retain records with SESE decisions



# Summarize
summarize_data <- function(att_name,df){
  data_summary <- table(df[att_name]) %>% 
  rbind(100*table(df[att_name])/nrow(df))
  rownames(data_summary) <- c("Total","Percent")
  print(att_name)
  print(round(data_summary,2))
}

summarize_data("COMPLEXITY",data_510k)


data_summary <- data_510k %>%
  mutate(MARK_QUARTER = floor_date(DECISIONDATE,"quarter")) %>%
  group_by(MARK_QUARTER, COMPLEXITY) %>%
  summarise(counts = n())

ggplot(data_summary,aes(MARK_QUARTER, counts, group = COMPLEXITY))+
  geom_line(aes(color = COMPLEXITY))+
  geom_point(aes(color = COMPLEXITY))+
  scale_color_discrete()+
  theme_bw()+
  theme(panel.background = element_rect(fill = "#f7edfa"))

```

It appears all classes have representation across the time-axis. Interesting to note that number of high complexity submissions have been increasing over time.


## Explore and engineer potential features that could be useful for the problem


For this analysis, use a portion of data up to 2017 (considering a potential training set along the time axis), in order to avoid using entire data for exploration.


### PRODUCTCODE Perspective

Are there any PRODUCTCODE categories that could be predictive of COMPLEXITY?


```{r feature engineering PRODUCTCODE Perspective, fig.height= 6, fig.width=15}
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

################################################################################
# established processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS),
         COMPLEXITY = ifelse(DECISION_TIME_DAYS <= 90,"L",
                             ifelse(DECISION_TIME_DAYS <= 265,"M","H")) # Define COMPLEXITY based on DECISION_TIME_DAYS
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") # Retain records with SESE decisions
################################################################################

# Data for exploration (<=2017)
data_explore <- data_510k %>% filter(year(DECISIONDATE) <= 2017)

data_summary <- data_explore %>%
  group_by(PRODUCTCODE, COMPLEXITY) %>%
  summarise(counts_complexity = n()) %>%
  left_join(data_explore %>%
              group_by(PRODUCTCODE) %>%
              summarise(counts_total = n())
              ) %>%
  mutate(compexity_total_ratio = counts_complexity/counts_total) %>%
  filter(counts_total >=100) %>% # Only focus on product codes with 100 or more submissions observed within this exploratory data set
  left_join(data_explore %>%
              group_by(COMPLEXITY) %>%
              summarise(class_prevalance = n()/nrow(data_explore))
              ) %>%
  mutate(observed_minus_expected = compexity_total_ratio - class_prevalance,
         abs_observed_minus_expected = abs(observed_minus_expected))

ggplot(data_summary,aes(reorder(PRODUCTCODE,abs_observed_minus_expected), observed_minus_expected))+
  geom_bar(aes(fill = COMPLEXITY), stat = "identity")+
  scale_fill_discrete()+
  coord_flip()+
  facet_wrap(. ~ COMPLEXITY, scales = "free")+
  theme_bw()+
  theme(panel.background = element_rect(fill = "#f7edfa"))

hist(data_summary$abs_observed_minus_expected, breaks = 20, col = "navy")

# Potential PRODUCTCODE features using a threshold of abs_observed_minus_expected >= 0.2
product_code_influencers <- data_summary %>% filter(abs_observed_minus_expected >= 0.2) %>% 
  select(PRODUCTCODE) %>% distinct()
print(paste0(paste0(product_code_influencers$PRODUCTCODE, collapse = " "),
             " are product codes that could be predictive for 510(k) COMPLEXITY."))

# Generate binary/dummy variables representing these product codes

data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

################################################################################
# established processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS),
         COMPLEXITY = ifelse(DECISION_TIME_DAYS <= 90,"L",
                             ifelse(DECISION_TIME_DAYS <= 265,"M","H")) # Define COMPLEXITY based on DECISION_TIME_DAYS
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") %>% # Retain records with SESE decisions
  mutate(PRODUCTCODE_IYE = ifelse(PRODUCTCODE == "IYE",1,0),
         PRODUCTCODE_IYN = ifelse(PRODUCTCODE == "IYN",1,0),
         PRODUCTCODE_JJX = ifelse(PRODUCTCODE == "JJX",1,0),
         PRODUCTCODE_LYZ = ifelse(PRODUCTCODE == "LYZ",1,0),
         PRODUCTCODE_NBW = ifelse(PRODUCTCODE == "NBW",1,0),
         )
################################################################################


```


### CLASSADVISECOMM Perspective

Are there any CLASSADVISECOMM categories that could be predictive of COMPLEXITY?



```{r feature engineering CLASSADVISECOMM Perspective, fig.height= 6, fig.width=15}
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

################################################################################
# established processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS),
         COMPLEXITY = ifelse(DECISION_TIME_DAYS <= 90,"L",
                             ifelse(DECISION_TIME_DAYS <= 265,"M","H")) # Define COMPLEXITY based on DECISION_TIME_DAYS
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") %>% # Retain records with SESE decisions
  mutate(PRODUCTCODE_IYE = ifelse(PRODUCTCODE == "IYE",1,0),
         PRODUCTCODE_IYN = ifelse(PRODUCTCODE == "IYN",1,0),
         PRODUCTCODE_JJX = ifelse(PRODUCTCODE == "JJX",1,0),
         PRODUCTCODE_LYZ = ifelse(PRODUCTCODE == "LYZ",1,0),
         PRODUCTCODE_NBW = ifelse(PRODUCTCODE == "NBW",1,0),
         )
################################################################################

# Data for exploration (<=2017)
data_explore <- data_510k %>% filter(year(DECISIONDATE) <= 2017)

data_summary <- data_explore %>%
  group_by(CLASSADVISECOMM, COMPLEXITY) %>%
  summarise(counts_complexity = n()) %>%
  left_join(data_explore %>%
              group_by(CLASSADVISECOMM) %>%
              summarise(counts_total = n())
              ) %>%
  mutate(compexity_total_ratio = counts_complexity/counts_total) %>%
  filter(counts_total >=100) %>% # Only focus on CLASSADVISECOMM with 100 or more submissions observed within this exploratory data set
  left_join(data_explore %>%
              group_by(COMPLEXITY) %>%
              summarise(class_prevalance = n()/nrow(data_explore))
              ) %>%
  mutate(observed_minus_expected = compexity_total_ratio - class_prevalance,
         abs_observed_minus_expected = abs(observed_minus_expected))

ggplot(data_summary,aes(reorder(CLASSADVISECOMM,abs_observed_minus_expected), observed_minus_expected))+
  geom_bar(aes(fill = COMPLEXITY), stat = "identity")+
  scale_fill_discrete()+
  coord_flip()+
  facet_wrap(. ~ COMPLEXITY, scales = "free")+
  theme_bw()+
  theme(panel.background = element_rect(fill = "#f7edfa"))

hist(data_summary$abs_observed_minus_expected, breaks = 20, col = "navy")

# Potential CLASSADVISECOMM features using a threshold of abs_observed_minus_expected >= 0.15
product_code_influencers <- data_summary %>% filter(abs_observed_minus_expected >= 0.15) %>% 
  select(CLASSADVISECOMM) %>% distinct()
print(paste0(paste0(product_code_influencers$CLASSADVISECOMM, collapse = " "),
             " are CLASSADVISECOMM that could be predictive for 510(k) COMPLEXITY."))

# Generate binary/dummy variables representing these CLASSADVISECOMM

data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

################################################################################
# established processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS),
         COMPLEXITY = ifelse(DECISION_TIME_DAYS <= 90,"L",
                             ifelse(DECISION_TIME_DAYS <= 265,"M","H")) # Define COMPLEXITY based on DECISION_TIME_DAYS
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") %>% # Retain records with SESE decisions
  mutate(PRODUCTCODE_IYE = ifelse(PRODUCTCODE == "IYE",1,0),
         PRODUCTCODE_IYN = ifelse(PRODUCTCODE == "IYN",1,0),
         PRODUCTCODE_JJX = ifelse(PRODUCTCODE == "JJX",1,0),
         PRODUCTCODE_LYZ = ifelse(PRODUCTCODE == "LYZ",1,0),
         PRODUCTCODE_NBW = ifelse(PRODUCTCODE == "NBW",1,0),
         )%>%
  mutate(CLASSADVISECOMM_AN = ifelse(PRODUCTCODE == "AN",1,0),
         CLASSADVISECOMM_HE = ifelse(PRODUCTCODE == "HE",1,0),
         CLASSADVISECOMM_IM = ifelse(PRODUCTCODE == "IM",1,0),
         CLASSADVISECOMM_MI = ifelse(PRODUCTCODE == "MI",1,0),
         CLASSADVISECOMM_RA = ifelse(PRODUCTCODE == "RA",1,0),
         CLASSADVISECOMM_TX = ifelse(PRODUCTCODE == "TX",1,0),
  )
################################################################################


```

### DEVICENAME Perspective


Are there any terms from DEVICENAME text that could be considered potentially predictive for COMPLEXITY?


```{r feature engineering DEVICENAME Perspective, fig.height= 6, fig.width=1, eval=FALSE}
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

################################################################################
# established processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS),
         COMPLEXITY = ifelse(DECISION_TIME_DAYS <= 90,"L",
                             ifelse(DECISION_TIME_DAYS <= 265,"M","H")) # Define COMPLEXITY based on DECISION_TIME_DAYS
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") %>% # Retain records with SESE decisions
  mutate(PRODUCTCODE_IYE = ifelse(PRODUCTCODE == "IYE",1,0),
         PRODUCTCODE_IYN = ifelse(PRODUCTCODE == "IYN",1,0),
         PRODUCTCODE_JJX = ifelse(PRODUCTCODE == "JJX",1,0),
         PRODUCTCODE_LYZ = ifelse(PRODUCTCODE == "LYZ",1,0),
         PRODUCTCODE_NBW = ifelse(PRODUCTCODE == "NBW",1,0),
         )%>%
  mutate(CLASSADVISECOMM_AN = ifelse(PRODUCTCODE == "AN",1,0),
         CLASSADVISECOMM_HE = ifelse(PRODUCTCODE == "HE",1,0),
         CLASSADVISECOMM_IM = ifelse(PRODUCTCODE == "IM",1,0),
         CLASSADVISECOMM_MI = ifelse(PRODUCTCODE == "MI",1,0),
         CLASSADVISECOMM_RA = ifelse(PRODUCTCODE == "RA",1,0),
         CLASSADVISECOMM_TX = ifelse(PRODUCTCODE == "TX",1,0),
  )
################################################################################

# Data for exploration (<=2017)
data_explore <- data_510k %>% filter(year(DECISIONDATE) <= 2017)

data_summary <- data_explore %>% select(COMPLEXITY,DEVICENAME)
# Tokenize DEVICENAME and convert to document term frequency matrix
dtm <- dfm(tokens(data_summary$DEVICENAME, remove_punct = TRUE), remove_padding = TRUE) 
# calculate and filter out near-zero variance features
dtm_nzv <- list()
start_i <- 1
end_i <- 5000

# Long operation
for (i in 1:6){
  dtm_nzv[[i]] <- nzv(dtm[,start_i:end_i]) 
  start_i <- start_i + 5000
  end_i <- end_i + 5000
  print(paste0("Searched up to feature: ",end_i))
}
w <- sapply(dtm_nzv, length)
w <- which(w < 5000)
dtm_select <- dtm[,c(1:5000)[-dtm_nzv[[w]]]] 
dtm_select <- convert(dtm_select, to = "data.frame") 

# Check the relative prevalence of complexity classes where these terms appear
data_summary <- cbind(data_summary %>% select(-DEVICENAME),
                      dtm_select %>% select(-doc_id))

data_summary_freq <- data_summary %>%
  melt() %>%
  filter(value != 0) %>%
  group_by(COMPLEXITY,variable) %>%
  summarise(total_count_variable_and_complexity = n()) %>% 
  left_join(data_summary %>%
    melt() %>%
    filter(value != 0) %>%
    group_by(variable) %>%
    summarise(total_count_variable = n()) ) %>%
  mutate(observed_freq_complexity_in_variable = total_count_variable_and_complexity/total_count_variable ) %>%
  left_join(data_summary %>%
              group_by(COMPLEXITY) %>%
              summarise(expected_freq_complexity = n()/nrow(data_summary)))%>%
  mutate(observed_minus_expected = observed_freq_complexity_in_variable - expected_freq_complexity)
```


None of the terms I analyzed from the device names displayed a noticeable enrichment of any COMPLEXITY class.


### Do Principal Component Space of DEVICENAME document frequency matrix have any classification utility for COMPLEXITY?


```{r feature engineering DEVICENAME Perspective PCA, fig.height= 10, fig.width=10, eval=FALSE}
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

################################################################################
# established processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS),
         COMPLEXITY = ifelse(DECISION_TIME_DAYS <= 90,"L",
                             ifelse(DECISION_TIME_DAYS <= 265,"M","H")) # Define COMPLEXITY based on DECISION_TIME_DAYS
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") %>% # Retain records with SESE decisions
  mutate(PRODUCTCODE_IYE = ifelse(PRODUCTCODE == "IYE",1,0),
         PRODUCTCODE_IYN = ifelse(PRODUCTCODE == "IYN",1,0),
         PRODUCTCODE_JJX = ifelse(PRODUCTCODE == "JJX",1,0),
         PRODUCTCODE_LYZ = ifelse(PRODUCTCODE == "LYZ",1,0),
         PRODUCTCODE_NBW = ifelse(PRODUCTCODE == "NBW",1,0),
         )%>%
  mutate(CLASSADVISECOMM_AN = ifelse(PRODUCTCODE == "AN",1,0),
         CLASSADVISECOMM_HE = ifelse(PRODUCTCODE == "HE",1,0),
         CLASSADVISECOMM_IM = ifelse(PRODUCTCODE == "IM",1,0),
         CLASSADVISECOMM_MI = ifelse(PRODUCTCODE == "MI",1,0),
         CLASSADVISECOMM_RA = ifelse(PRODUCTCODE == "RA",1,0),
         CLASSADVISECOMM_TX = ifelse(PRODUCTCODE == "TX",1,0),
  )
################################################################################

# Data for exploration (<=2017)
data_explore <- data_510k %>% filter(year(DECISIONDATE) <= 2017)

data_summary <- data_explore %>% select(COMPLEXITY,DEVICENAME)
# Tokenize DEVICENAME and convert to document term frequency matrix
gc()
memory.limit(size= 60000)
dtm <- dfm(tokens(data_summary$DEVICENAME, remove_punct = TRUE), remove_padding = TRUE)

# Select top 200 features
top_200 <- names(topfeatures(dtm, n = 200))
dtm_top200 <- dtm[,top_200] %>% convert(to = "data.frame")

gc()
memory.limit(size= 1200000)
dtm_pca <- prcomp(dtm_top200 %>% select(-doc_id), scale. = TRUE) 
pcs <- dtm_pca$x

colors_pca <- ifelse(data_summary$COMPLEXITY == "H","red", 
                     ifelse(data_summary$COMPLEXITY == "L", "green","blue"))

pairs(pcs[,1:5], pch = 16, cex = .75, 
      col = scales::alpha(colors_pca,.2))

pairs(pcs[,c(1,2)], pch = 16, cex = .75, 
      col = scales::alpha(colors_pca,.2))

```

PCA generated features from DEVICENAME don't appear to be effective for this classification task.


### APPLICANT perspective


Do certain APPLICANT names display utility for COMPLEXITY classification?


```{r feature engineering APPLICANT Perspective, fig.height= 18, fig.width=15, eval=FALSE}
data_510k <- read.csv(paste0("../data/",data.filename), sep = "|", stringsAsFactors = FALSE)

################################################################################
# established processing steps
data_510k <- data_510k %>%
  mutate(DATERECEIVED = mdy(DATERECEIVED),
         DECISIONDATE = mdy(DECISIONDATE),
         DECISION_TIME_DAYS = as.numeric(DECISIONDATE - DATERECEIVED),# define DECISION_TIME_DAYS
         DECISION_TIME_DAYS_LOG10 = log10(DECISION_TIME_DAYS),
         COMPLEXITY = ifelse(DECISION_TIME_DAYS <= 90,"L",
                             ifelse(DECISION_TIME_DAYS <= 265,"M","H")) # Define COMPLEXITY based on DECISION_TIME_DAYS
         ) %>%
  filter(DECISIONDATE >= mdy("10-01-2007")) %>% # Filter >= FY 2007 (MDUFA II)
  filter(THIRDPARTY == 'N' & # Filter out 3rd party submissions
         TYPE == "Traditional" & # Retain Traditional 510(k)s
         DECISION == "SESE") %>% # Retain records with SESE decisions
  mutate(PRODUCTCODE_IYE = ifelse(PRODUCTCODE == "IYE",1,0),
         PRODUCTCODE_IYN = ifelse(PRODUCTCODE == "IYN",1,0),
         PRODUCTCODE_JJX = ifelse(PRODUCTCODE == "JJX",1,0),
         PRODUCTCODE_LYZ = ifelse(PRODUCTCODE == "LYZ",1,0),
         PRODUCTCODE_NBW = ifelse(PRODUCTCODE == "NBW",1,0),
         )%>%
  mutate(CLASSADVISECOMM_AN = ifelse(PRODUCTCODE == "AN",1,0),
         CLASSADVISECOMM_HE = ifelse(PRODUCTCODE == "HE",1,0),
         CLASSADVISECOMM_IM = ifelse(PRODUCTCODE == "IM",1,0),
         CLASSADVISECOMM_MI = ifelse(PRODUCTCODE == "MI",1,0),
         CLASSADVISECOMM_RA = ifelse(PRODUCTCODE == "RA",1,0),
         CLASSADVISECOMM_TX = ifelse(PRODUCTCODE == "TX",1,0),
  )
################################################################################

# Data for exploration (<=2017)
data_explore <- data_510k %>% filter(year(DECISIONDATE) <= 2017)

data_summary <- data_explore %>% select(COMPLEXITY,APPLICANT)
# Tokenize APPLICANT and convert to document term frequency matrix
gc()
memory.limit(size= 60000)
dtm <- dfm(tokens(data_summary$APPLICANT, remove_punct = TRUE))

# Select top 200 features
top_200 <- names(topfeatures(dtm, n = 200))
dtm_top200 <- dtm[,top_200] %>% convert(to = "data.frame")

# Check the relative prevalence of complexity classes where these terms appear
data_summary <- cbind(data_summary %>% select(-APPLICANT),
                      dtm_top200 %>% select(-doc_id))

data_summary_freq <- data_summary %>%
  melt() %>%
  filter(value != 0) %>%
  group_by(COMPLEXITY,variable) %>%
  summarise(total_count_variable_and_complexity = n()) %>% 
  left_join(data_summary %>%
    melt() %>%
    filter(value != 0) %>%
    group_by(variable) %>%
    summarise(total_count_variable = n()) ) %>%
  mutate(observed_freq_complexity_in_variable = total_count_variable_and_complexity/total_count_variable ) %>%
  left_join(data_summary %>%
              group_by(COMPLEXITY) %>%
              summarise(expected_freq_complexity = n()/nrow(data_summary)))%>%
  mutate(observed_minus_expected = observed_freq_complexity_in_variable - expected_freq_complexity) %>%
  mutate(abs_observed_minus_expected = abs(observed_minus_expected))

hist(data_summary_freq$observed_minus_expected, breaks = 30, col = "navy")


ggplot(data_summary_freq,aes(reorder(variable,abs_observed_minus_expected), observed_minus_expected))+
  geom_bar(aes(fill = COMPLEXITY), stat = "identity")+
  scale_fill_discrete()+
  coord_flip()+
  geom_hline(yintercept = 0.2, color = "navy", linetype = "dashed")+
  geom_hline(yintercept = -0.2, color = "navy", linetype = "dashed")+
  facet_wrap(. ~ COMPLEXITY, scales = "free")+
  theme_bw()+
  theme(panel.background = element_rect(fill = "#f7edfa"))

print(data_summary_freq %>% filter(abs_observed_minus_expected >= .2))
```


These terms are in very low frequency to infer whether their enrichment may be generalizeable or not. 


## Next steps


- Prepare an automation script for data ingestion and preparation of the analytical data using the observations described here.
- Make decisions on a modeling and validation framework (i.e: define training, validation and test sets).
- Initiate experimentation phase: start building benchmark classifiers (just by minimal data integration and feature engineering) using the training set and estimate cross-validated performance.
- Explore integrating additional data resources (e.g: summary/statements to uplift benchmark model performance)



